{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualizing the data at different instances during the processing\n",
    "\n",
    "- Without any processing\n",
    "- After processing\n",
    "    - PCA to bring down to 50\n",
    "    - t-sne for 2D visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Before Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From the local training data set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from lib import data_prep, feature_extraction\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from lib.data_loader_dynamodb import DynamoDBDataLoader\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 22\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Base directory\n",
    "data_loc = os.path.join(os.getcwd(), \"DATA\")\n",
    "\n",
    "# file name\n",
    "file_names = {\n",
    "    0: \"machine_ON_no-ref_start-error_1.csv\",  # Machine turned ON, and the parameter switch enable error\n",
    "    1: \"machine_ON_no-ref_start-error_2.csv\",\n",
    "    2: \"machine_ON_no-ref_start-error_3.csv\",\n",
    "    3: \"machine_ON_no-ref_start-error_4.csv\",\n",
    "    4: \"machine_ON_ref_no-error_1.csv\",  # Machine ON referenced and no-error idling\n",
    "    5: \"machine_ON_ref_no-error_2.csv\",  # Machine ON referenced and no-error idling\n",
    "    6: \"machine_ON_ref_no-error_3.csv\",\n",
    "    7: \"machine_ON_ref_no-error_4.csv\",\n",
    "    8: \"machine_ON_ref_no-error_5.csv\",\n",
    "    9: \"machine_ON_ref_no-error_6.csv\",\n",
    "    10: \"machine_ON_ref_no-error_7.csv\",\n",
    "    11: \"machine_ON_ref_no-error_8.csv\",\n",
    "    12: \"machine_ON_ref_no-error_9.csv\",\n",
    "    13: \"machine_ON_ref_no-error_10.csv\",\n",
    "    14: \"machine_ON_ref_no-error_11.csv\",\n",
    "    15: \"machine_ON_ref_no-error_12.csv\",\n",
    "    16: \"machine_ON_ref_no-error_13.csv\",\n",
    "    17: \"machine_ON_ref_overtravel-error_x_neg_1.csv\",  # Machine ON referenced and Overtravel for X negative\n",
    "    18: \"machine_ON_ref_overtravel-error_x_pos_1.csv\",  # Machine ON referenced and Overtravel for X positive\n",
    "    19: \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\",  # Machine ON not-referenced and Overtravel for X negative\n",
    "    20: \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", # Machine ON not-referenced and Overtravel for X positive\n",
    "    21: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\", # Reference and overtravel in X\n",
    "    22: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", # Referenced and overtravel in X\n",
    "    23: \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\", # Referenced and overtravel in X\n",
    "    24: \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\",  # Machine ON referenced and Overtravel for Y negative\n",
    "    25: \"machine_ON_ref_overtravel-error_y_neg_1.csv\", # Machine and ON referenced and Overtravel in Y\n",
    "    26: \"machine_ON_ref_overtravel-error_y_pos_1.csv\",  # Machine ON referenced and Overtravel for Y positive\n",
    "    27: \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\",\n",
    "    28: \"machine_ON_ref_overtravel-error_z_neg_1.csv\",  # Machine ON referenced and Overtravel for Z negative\n",
    "    29: \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\",\n",
    "    30: \"machine_ON_ref_overtravel-error_z_pos_1.csv\",  # Machine ON referenced and Overtravel for Z positive\n",
    "    31: \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\",\n",
    "    32: \"machine_ON_no-ref_1.csv\",\n",
    "    33: \"machine_ON_no-ref_2.csv\"\n",
    "}\n",
    "\n",
    "# Other file names that were used\n",
    "other_file_names = {\n",
    "    0: \"components_on-off.csv\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "index = 4\n",
    "df = pd.read_csv(os.path.join(data_loc, file_names[index]), header=\"infer\", index_col=\"no\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "x = np.arange(60)\n",
    "\n",
    "# Plot\n",
    "axs.plot(x, df[\"PowerSum\"][-60:], linewidth=5, color=\"black\")\n",
    "axs.hlines(df[\"PowerSum\"][-60:].mean(), xmin=0, xmax=60, colors=\"red\", linestyles=\"dashed\")\n",
    "axs.grid(False)\n",
    "\n",
    "# Augment the axis\n",
    "axs.set_xlabel(\"Time (s)\")\n",
    "axs.set_ylim(274, 276.5)\n",
    "axs.set_ylabel(\"Active Power (W)\")\n",
    "axs.set_title(\"Active Power consumed over 60s window\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Directly querying the DynamoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cutting power\n",
    "start_time = int(datetime(2022, 5, 1, 14, 22).timestamp())\n",
    "num_secs = 600\n",
    "end_time = start_time + num_secs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instance\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=\"us-east-1\")\n",
    "ddb = DynamoDBDataLoader(table_name=\"robonano1_energy_wn\", region=\"us-east-1\", dynamodb=dynamodb)\n",
    "\n",
    "# Query\n",
    "ddb.query_data(sample_time_range=(start_time, end_time))\n",
    "# Get the dataframe\n",
    "ddb.get_dataframe()\n",
    "# get data\n",
    "data = ddb.data\n",
    "\n",
    "# Choose appropriate columns\n",
    "chosen_cols = [\"PowerSum\", \"Power1\", \"Power2\", \"Power3\", \"PowerReac1\", \"PowerReac2\", \"PowerReac3\", \"PowerApp1\", \"PowerApp2\", \"PowerApp3\"]\n",
    "data = data[chosen_cols]\n",
    "backup_data = data\n",
    "data = data.to_numpy()[0:60, :][:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "x = np.arange(60)\n",
    "\n",
    "# Plot\n",
    "axs.plot(x, data, linewidth=5, color=\"black\")\n",
    "axs.hlines(data.mean(), xmin=0, xmax=60, colors=\"red\", linestyles=\"dashed\")\n",
    "axs.grid(False)\n",
    "\n",
    "# Augment the axis\n",
    "axs.set_xlabel(\"Time (s)\")\n",
    "axs.set_ylim(274, 276.5)\n",
    "axs.set_ylabel(\"Active Power (W)\")\n",
    "axs.set_title(\"Active Power consumed over 60s window\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PLot the whole range\n",
    "data = backup_data.to_numpy()[90:300][:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "x = np.arange(data.shape[0])\n",
    "\n",
    "# Plot\n",
    "axs.plot(x, np.log(data), linewidth=5, color=\"black\")\n",
    "axs.grid(False)\n",
    "axs.set_ylabel(\"Log of Active Power (W)\")\n",
    "axs.set_title(\"Active Power consumption\")\n",
    "axs.set_xlabel(\"Time (s)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## After Processing\n",
    "\n",
    "- Visualization the data after some amount of processing has been conducted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preparation\n",
    "\n",
    "- Segmentation\n",
    "- Feature extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_secs = 60\n",
    "wavelet_nperseg = 15\n",
    "overlap_rate = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dont choose \"no\" and \"sample_time\" as they will be added later to the beginning\n",
    "# Chosen - Three different power components for three phases\n",
    "chosen_cols = [\"Power1\", \"Power2\", \"Power3\", \"PowerReac1\", \"PowerReac2\", \"PowerReac3\", \"PowerApp1\", \"PowerApp2\", \"PowerApp3\"]\n",
    "segmented_data = {}\n",
    "for index, file_name in file_names.items():\n",
    "    path = os.path.join(data_loc, file_name)\n",
    "    temp = data_prep.segment_data(file_name=path, col_names=chosen_cols, segment_secs=segment_secs, overlap_rate=overlap_rate)\n",
    "    # Remove the sample_time col\n",
    "    temp = temp[:, 1:, :]\n",
    "    segmented_data[file_name] =  temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that segmentation is successful\n",
    "for file_name in segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"For the file-{file_name} the shape-{segmented_data[file_name].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Associations between the classes and the files in this study\n",
    "# TODO: Separate the no-ref ON no-error class into a separate one?\n",
    "class_file_association = {\n",
    "    \"on-ref\": [\"machine_ON_ref_no-error_1.csv\", \"machine_ON_ref_no-error_2.csv\", \"machine_ON_ref_no-error_3.csv\", \"machine_ON_ref_no-error_4.csv\", \"machine_ON_ref_no-error_5.csv\", \"machine_ON_ref_no-error_6.csv\", \"machine_ON_ref_no-error_7.csv\", \"machine_ON_ref_no-error_8.csv\", \"machine_ON_ref_no-error_9.csv\", \"machine_ON_ref_no-error_10.csv\",\n",
    "               \"machine_ON_ref_no-error_11.csv\", \"machine_ON_ref_no-error_12.csv\", \"machine_ON_ref_no-error_13.csv\"],# \"machine_ON_no-ref_1.csv\", \"machine_ON_no-ref_2.csv\"],\n",
    "\n",
    "    \"on-noref-error\": [\"machine_ON_no-ref_start-error_1.csv\", \"machine_ON_no-ref_start-error_2.csv\", \"machine_ON_no-ref_start-error_3.csv\", \"machine_ON_no-ref_start-error_4.csv\"],\n",
    "\n",
    "    \"overtravel-x\": [\"machine_ON_ref_overtravel-error_x_neg_1.csv\", \"machine_ON_ref_overtravel-error_x_pos_1.csv\", \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\", \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\",\n",
    "    \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-y\": [\"machine_ON_ref_overtravel-error_y_neg_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_1.csv\",\n",
    "                    \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-z\": [\"machine_ON_ref_overtravel-error_z_neg_1.csv\", \"machine_ON_ref_overtravel-error_z_pos_1.csv\", \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\"] # , \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Okay\n",
    "class_segmented_data = {}\n",
    "for class_instance in class_file_association.keys():\n",
    "    for index, file_name in enumerate(class_file_association[class_instance]):\n",
    "\n",
    "        if index == 0:\n",
    "            class_segmented_data[class_instance] = segmented_data[file_name]\n",
    "        else:\n",
    "            class_segmented_data[class_instance] = np.append(class_segmented_data[class_instance], segmented_data[file_name], axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape the data appropriately\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    class_segmented_data[class_instance] = np.transpose(class_segmented_data[class_instance], (2, 1, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that the files have been loaded correctly\n",
    "for class_instance in class_segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"The class-{class_instance} has the shape-{class_segmented_data[class_instance].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_dataset_features = {}\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    dataset_features = []\n",
    "    for row in class_segmented_data[class_instance]:\n",
    "        computed_features = []\n",
    "        for col in row:\n",
    "            freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": wavelet_nperseg}]\n",
    "            freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "            # Concat features\n",
    "            computed_features += feature_extraction.compute_all_features(col, freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "\n",
    "        # Append to a list\n",
    "        dataset_features.append(computed_features)\n",
    "\n",
    "    # Add to class instance\n",
    "    class_dataset_features[class_instance] = np.array(dataset_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sys.stdout.write(\"After feature extraction process\\n\\n\")\n",
    "for class_instance in class_dataset_features.keys():\n",
    "\n",
    "    sys.stdout.write(f'For the class-{class_instance} , the extracted features has the shape={class_dataset_features[class_instance].shape}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_label_associations = {\n",
    "    \"on-ref\": 0,\n",
    "    \"on-noref-error\": 1,\n",
    "    \"overtravel-x\": 2,\n",
    "    \"overtravel-y\": 3,\n",
    "    \"overtravel-z\": 4,\n",
    "}\n",
    "for index, class_instance in enumerate(class_dataset_features.keys()):\n",
    "\n",
    "    temp_X = class_dataset_features[class_instance]\n",
    "    temp_y = np.repeat(class_label_associations[class_instance], temp_X.shape[0])[:, np.newaxis]\n",
    "\n",
    "    if index == 0:\n",
    "        X = temp_X\n",
    "        y = temp_y\n",
    "    else:\n",
    "        X = np.append(X, temp_X, axis=0)\n",
    "        y = np.append(y, temp_y, axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "# To a vector format\n",
    "y =  np.squeeze(y)\n",
    "\n",
    "sys.stdout.write(f\"The final combined shape-{X.shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Augmentation - by oversampling\n",
    "- Oversample the normal scenario using SMOTE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "#\n",
    "# sm = SMOTE(random_state=42, sampling_strategy={0: 40000})\n",
    "# X, y = sm.fit_resample(X, y)\n",
    "#\n",
    "# print(f\"The shape after resampling the majority class - {X.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Standardize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dimension Reduction\n",
    "\n",
    "- Using PCA to number of components to 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the object\n",
    "pca = PCA(n_components=50, svd_solver=\"full\")\n",
    "# Fit and transform data\n",
    "X_pca = pca.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization\n",
    "\n",
    "After a another dimension reduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30.0, early_exaggeration=24, n_iter=10000, n_iter_without_progress=1000, init=\"pca\", verbose=1, learning_rate=\"auto\")\n",
    "# Fit and transform\n",
    "X_tnse = tsne.fit_transform(X_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modify y\n",
    "label_names = {0: \"normal\", 1: \"no-ref error\", 2: \"overtravel-x\", 3: \"overtravel-y\", 4: \"overtravel-z\"}\n",
    "y_str = [label_names[x] for x in y]\n",
    "\n",
    "# Scatterplot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# Convert to df\n",
    "X_tnse_df = pd.DataFrame(X_tnse, columns=[\"D1\", \"D2\"])\n",
    "sns.scatterplot(x='D1', y=\"D2\", hue=y_str, data=X_tnse_df, ax=axs, palette=\"deep\")\n",
    "# Modify labels\n",
    "axs.set_xlabel(\"Dimension-1\")\n",
    "axs.set_ylabel(\"Dimension-2\")\n",
    "axs.set_title(\"t-sne visualization\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Differentiating the overtravels seems to be a difficult task and it would potentially need more data to achieve that.\n",
    "- The class \"normal\" and class \"no-ref error\" seems to have very distinct pattern that can be easily seperated.\n",
    "- Maybe more data on the overtravel instances might help?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
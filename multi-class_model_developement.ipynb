{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model development for multi-class of faults"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from lib import data_prep, feature_extraction, models\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "font = {\n",
    "    \"family\": \"sans-serif\",\n",
    "    \"weight\": \"bold\",\n",
    "    \"size\": 16\n",
    "}\n",
    "matplotlib.rc(\"font\", **font)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading and testing the data\n",
    "- Plotting to ensure validity\n",
    "- Add more files to the training data as required"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Base directory\n",
    "data_loc = os.path.join(os.getcwd(), \"DATA\")\n",
    "\n",
    "# file name\n",
    "file_names = {\n",
    "    0: \"machine_ON_no-ref_start-error_1.csv\",  # Machine turned ON, and the parameter switch enable error\n",
    "    1: \"machine_ON_no-ref_start-error_2.csv\",\n",
    "    2: \"machine_ON_no-ref_start-error_3.csv\",\n",
    "    3: \"machine_ON_no-ref_start-error_4.csv\",\n",
    "    4: \"machine_ON_ref_no-error_1.csv\",  # Machine ON referenced and no-error idling\n",
    "    5: \"machine_ON_ref_no-error_2.csv\",  # Machine ON referenced and no-error idling\n",
    "    6: \"machine_ON_ref_no-error_3.csv\",\n",
    "    7: \"machine_ON_ref_no-error_4.csv\",\n",
    "    8: \"machine_ON_ref_no-error_5.csv\",\n",
    "    9: \"machine_ON_ref_no-error_6.csv\",\n",
    "    10: \"machine_ON_ref_no-error_7.csv\",\n",
    "    11: \"machine_ON_ref_no-error_8.csv\",\n",
    "    12: \"machine_ON_ref_no-error_9.csv\",\n",
    "    13: \"machine_ON_ref_no-error_10.csv\",\n",
    "    14: \"machine_ON_ref_overtravel-error_x_neg_1.csv\",  # Machine ON referenced and Overtravel for X negative\n",
    "    15: \"machine_ON_ref_overtravel-error_x_pos_1.csv\",  # Machine ON referenced and Overtravel for X positive\n",
    "    16: \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\",  # Machine ON not-referenced and Overtravel for X negative\n",
    "    17: \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", # Machine ON not-referenced and Overtravel for X positive\n",
    "    18: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\", # Reference and overtravel in X\n",
    "    19: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", # Referenced and overtravel in X\n",
    "    20: \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\", # Referenced and overtravel in X\n",
    "    21: \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\",  # Machine ON referenced and Overtravel for Y negative\n",
    "    22: \"machine_ON_ref_overtravel-error_y_neg_1.csv\", # Machine and ON referenced and Overtravel in Y\n",
    "    23: \"machine_ON_ref_overtravel-error_y_pos_1.csv\",  # Machine ON referenced and Overtravel for Y positive\n",
    "    24: \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\",\n",
    "    25: \"machine_ON_ref_overtravel-error_z_neg_1.csv\",  # Machine ON referenced and Overtravel for Z negative\n",
    "    26: \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\",\n",
    "    27: \"machine_ON_ref_overtravel-error_z_pos_1.csv\",  # Machine ON referenced and Overtravel for Z positive\n",
    "    28: \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\",\n",
    "    29: \"machine_ON_no-ref_1.csv\",\n",
    "    30: \"machine_ON_no-ref_2.csv\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "index = 5\n",
    "df = pd.read_csv(os.path.join(data_loc, file_names[index]), header=\"infer\", index_col=\"no\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "df[\"PowerSum\"][-120:].plot(ax=axs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation\n",
    "\n",
    "- Control different segmentation length\n",
    "    - 2 mins\n",
    "    - 1 min\n",
    "    - 30 secs\n",
    "    - 15 secs\n",
    "\n",
    "- Refer to the spreadsheet in the results directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_secs = 60\n",
    "wavelet_nperseg = 15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dont choose \"no\" and \"sample_time\" as they will be added later to the beginning\n",
    "# Chosen - Three different power components for three phases\n",
    "chosen_cols = [\"Power1\", \"Power2\", \"Power3\", \"PowerReac1\", \"PowerReac2\", \"PowerReac3\", \"PowerApp1\", \"PowerApp2\", \"PowerApp3\"]\n",
    "segmented_data = {}\n",
    "for index, file_name in file_names.items():\n",
    "    path = os.path.join(data_loc, file_name)\n",
    "    temp = data_prep.segment_data(file_name=path, col_names=chosen_cols, segment_secs=segment_secs)\n",
    "    # Remove the sample_time col\n",
    "    temp = temp[:, 1:, :]\n",
    "    segmented_data[file_name] =  temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that segmentation is successful\n",
    "for file_name in segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"For the file-{file_name} the shape-{segmented_data[file_name].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine classes\n",
    "\n",
    "Make a choice on the number of classes to be used for study"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Associations between the classes and the files in this study\n",
    "class_file_association = {\n",
    "    \"on-ref\": [\"machine_ON_ref_no-error_1.csv\", \"machine_ON_ref_no-error_2.csv\", \"machine_ON_ref_no-error_3.csv\", \"machine_ON_ref_no-error_4.csv\", \"machine_ON_ref_no-error_5.csv\", \"machine_ON_ref_no-error_6.csv\", \"machine_ON_ref_no-error_7.csv\", \"machine_ON_ref_no-error_8.csv\", \"machine_ON_ref_no-error_9.csv\", \"machine_ON_ref_no-error_9.csv\"],\n",
    "\n",
    "    \"on-noref-error\": [\"machine_ON_no-ref_start-error_1.csv\", \"machine_ON_no-ref_start-error_2.csv\", \"machine_ON_no-ref_start-error_3.csv\", \"machine_ON_no-ref_start-error_4.csv\", \"machine_ON_no-ref_1.csv\", \"machine_ON_no-ref_2.csv\"],\n",
    "\n",
    "    \"overtravel-x\": [\"machine_ON_ref_overtravel-error_x_neg_1.csv\", \"machine_ON_ref_overtravel-error_x_pos_1.csv\", \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\", \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\",\n",
    "    \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-y\": [\"machine_ON_ref_overtravel-error_y_neg_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_1.csv\",\n",
    "                    \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-z\": [\"machine_ON_ref_overtravel-error_z_neg_1.csv\", \"machine_ON_ref_overtravel-error_z_pos_1.csv\", \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\", \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Okay\n",
    "class_segmented_data = {}\n",
    "for class_instance in class_file_association.keys():\n",
    "    for index, file_name in enumerate(class_file_association[class_instance]):\n",
    "\n",
    "        if index == 0:\n",
    "            class_segmented_data[class_instance] = segmented_data[file_name]\n",
    "        else:\n",
    "            class_segmented_data[class_instance] = np.append(class_segmented_data[class_instance], segmented_data[file_name], axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape the data appropriately\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    class_segmented_data[class_instance] = np.transpose(class_segmented_data[class_instance], (2, 1, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that the files have been loaded correctly\n",
    "for class_instance in class_segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"The class-{class_instance} has the shape-{class_segmented_data[class_instance].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction\n",
    "\n",
    "- Time domain\n",
    "- Frequency domain\n",
    "- Time-frequency domain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features to extract\n",
    "features_to_select = input(\"Features to select\\n\\t--All features 'all'\\n\\t--Time domain features => 'time'\\n\\t--Frequency domain features => 'freq'\\n\\t--Time-Frequency domain features => 'time-freq'\\n--Choice: \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_dataset_features = {}\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    dataset_features = []\n",
    "    for row in class_segmented_data[class_instance]:\n",
    "        computed_features = []\n",
    "        for col in row:\n",
    "            freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": wavelet_nperseg}]\n",
    "            freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "            if features_to_select == 'all':\n",
    "                computed_features += feature_extraction.compute_all_features(col, freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "            elif features_to_select == 'time':\n",
    "                computed_features += feature_extraction.compute_time_domain_features(col)\n",
    "            elif features_to_select == 'freq':\n",
    "                computed_features += feature_extraction.compute_frequency_domain_features(col, args=freq_args)\n",
    "            elif features_to_select == \"time-freq\":\n",
    "                computed_features += feature_extraction.compute_time_frequency_features(col, args=freq_time_args)\n",
    "            else:\n",
    "                sys.exit(\"Unknown Choice on the feature selection\")\n",
    "\n",
    "        # Append to a list\n",
    "        dataset_features.append(computed_features)\n",
    "\n",
    "    # Add to class instance\n",
    "    class_dataset_features[class_instance] = np.array(dataset_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sys.stdout.write(\"After feature extraction process\\n\\n\")\n",
    "for class_instance in class_dataset_features.keys():\n",
    "\n",
    "    sys.stdout.write(f'For the class-{class_instance} , the extracted features has the shape={class_dataset_features[class_instance].shape}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate training data\n",
    "\n",
    "- Combine all with appropriate labels\n",
    "- Using KFold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_label_associations = {\n",
    "    \"on-ref\": 0,\n",
    "    \"on-noref-error\": 1,\n",
    "    \"overtravel-x\": 2,\n",
    "    \"overtravel-y\": 3,\n",
    "    \"overtravel-z\": 4\n",
    "}\n",
    "for index, class_instance in enumerate(class_dataset_features.keys()):\n",
    "\n",
    "    temp_X = class_dataset_features[class_instance]\n",
    "    temp_y = np.repeat(class_label_associations[class_instance], temp_X.shape[0])[:, np.newaxis]\n",
    "\n",
    "    if index == 0:\n",
    "        X = temp_X\n",
    "        y = temp_y\n",
    "    else:\n",
    "        X = np.append(X, temp_X, axis=0)\n",
    "        y = np.append(y, temp_y, axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "# To a vector format\n",
    "y =  np.squeeze(y)\n",
    "\n",
    "sys.stdout.write(f\"The final combined shape-{X.shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model development\n",
    "- Choose among the available models\n",
    "- Set the parameters appropriately\n",
    "- Train the model and get the metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-parameters optimization\n",
    "\n",
    "- trying to find the best hyperparameters for each of the models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Manually chosen ones\n",
    "model_params = {\n",
    "    \"LogisticRegression\" : {\"class_weight\": \"balanced\", \"max_iter\": 5000, \"multi_class\": \"multinomial\", \"n_jobs\": 4},\n",
    "    \"DecisionTreeClassifier\": {\"min_samples_split\": 100, \"class_weight\": \"balanced\"},\n",
    "    \"KNeighborsClassifier\": {\"n_neighbors\": 10},\n",
    "    \"SVC\": {\"kernel\": \"rbf\", \"tol\":1e-7, \"class_weight\": \"balanced\"},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": 50},\n",
    "    \"RandomForestClassifier\": {\"n_estimators\": 100, \"min_samples_split\": 100, \"class_weight\": \"balanced\"},\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Possible parameters\n",
    "hyper_params = {\n",
    "    \"LogisticRegression\" : {\"tol\": [0.0001, 0.00005, 0.0000005], \"max_iter\": [5000, 10000, 20000], \"multi_class\": [\"multinomial\"], \"n_jobs\": [4], \"class_weight\": [\"balanced\"]},\n",
    "    \"DecisionTreeClassifier\": {\"min_samples_split\": [100, 500], \"max_depth\": [None, 5, 10, 15, 50], \"min_samples_leaf\":[1, 100, 500], \"class_weight\": [\"balanced\"]},\n",
    "    \"KNeighborsClassifier\": {\"n_neighbors\": [10, 5, 20, 50, 100], \"weights\":[\"uniform\", \"distance\"]},\n",
    "    \"SVC\": {\"kernel\": [\"linear\", \"poly\", \"rbf\"], \"tol\":[1e-7, 1e-3], \"class_weight\": [\"balanced\"]},\n",
    "    \"BaggingClassifier\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"RandomForestClassifier\": {\"n_estimators\": [100, 50, 200], \"min_samples_split\": [100, 500], \"max_depth\": [None, 5, 10, 15, 50], \"min_samples_leaf\":[1, 100, 500], \"class_weight\": [\"balanced\"]},\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create repo of models for hyperparameter optimization\n",
    "models_repo_hyperopt = models.Models()\n",
    "# Initialize the models\n",
    "models_repo_hyperopt.create_models(model_params)\n",
    "\n",
    "# Optimize the hyper-parameters for all models\n",
    "models_repo_hyperopt.optimize_hyperparameters(hyperparameters=hyper_params, X_train=X, y_train=y, standardize=True)\n",
    "\n",
    "# Print the optimized f1-scores\n",
    "print(\"F1-Scores\")\n",
    "for model_name in models_repo_hyperopt.hyper_opt_model_scores.keys():\n",
    "\n",
    "    print(f\"{model_name} - {models_repo_hyperopt.hyper_opt_model_scores[model_name]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the best performing parameters\n",
    "for model_name in models_repo_hyperopt.hyper_opt_model_params.keys():\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    print(models_repo_hyperopt.hyper_opt_model_params[model_name])\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CV Fold training\n",
    "\n",
    "- Training across 10-fold CV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set this if you are optimizing before this step\n",
    "# model_params = models_repo_hyperopt.hyper_opt_model_params\n",
    "\n",
    "# The ones below are hardcoded optimized hyperparameters\n",
    "# for each of the models\n",
    "model_params = {'LogisticRegression': {'max_iter': 5000, 'multi_class': 'multinomial', 'n_jobs': 4, 'tol': 0.0001, \"class_weight\": \"balanced\"},\n",
    " 'DecisionTreeClassifier': {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 100},\n",
    " 'KNeighborsClassifier': {'n_neighbors': 20},\n",
    " 'SVC': {'class_weight': 'balanced', 'kernel': 'linear', 'tol': 1e-07},\n",
    " 'BaggingClassifier': {'n_estimators': 50},\n",
    " 'RandomForestClassifier': {'class_weight': 'balanced', 'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 100, 'n_estimators': 200}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create repo of models\n",
    "models_repo = models.Models()\n",
    "# Initialize the models\n",
    "models_repo.create_models(model_params)\n",
    "\n",
    "# 10-fold to determine the effective performance of all models\n",
    "# And standardize the data\n",
    "cv_results_summary = models_repo.train_models_cvfolds(X, y, summarize_results=True, standardize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Better way to set this\n",
    "model_association = {\n",
    "    0: \"LogisticRegression\",\n",
    "    1: \"DecisionTreeClassifier\",\n",
    "    2: \"KNeighborsClassifier\",\n",
    "    3: \"SVC\",\n",
    "    4: \"BaggingClassifier\",\n",
    "    5: \"RandomForestClassifier\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the required dataframe and record the metrics\n",
    "model_id = 5\n",
    "cv_results_summary[model_association[model_id]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save the data\n",
    "\n",
    "Just for future reference - can be used later to analyze the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_save_dir = os.path.join(os.getcwd(), \"results\")\n",
    "train_save_file_name = \"training_data.pkl\"\n",
    "\n",
    "# Training data\n",
    "train_data = {\n",
    "    \"X\": X,\n",
    "    \"y\": y\n",
    "}\n",
    "\n",
    "with open(os.path.join(train_save_dir, train_save_file_name), \"wb\") as file_handle:\n",
    "    pickle.dump(train_data, file_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model development for anomaly detection\n",
    "\n",
    "Generally, what we are trying to do is to estimate the density of the \"normal\" - \"good\" scenario\n",
    "\n",
    "Two approaches were followed\n",
    "\n",
    "- Estimating the density by identifying mean and covariance of the \"normal/good\" distribution. Then using Mahalanobis distance to see how far a data instance is from a distribution.\n",
    "- Kernel based density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "# Append library path\n",
    "lib_path = os.path.join(os.path.dirname(os.getcwd()), \"lib\")\n",
    "sys.path.append(lib_path)\n",
    "import numpy as np\n",
    "import boto3\n",
    "import time\n",
    "import data_prep, feature_extraction, model_evaluations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the data\n",
    "\n",
    "Adding all required files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Base data directory location\n",
    "data_loc = os.path.join(os.path.dirname(os.getcwd()), \"DATA\")\n",
    "\n",
    "# file name\n",
    "file_names = {\n",
    "    0: \"machine_ON_no-ref_start-error_1.csv\",  # Machine turned ON, and the parameter switch enable error\n",
    "    1: \"machine_ON_no-ref_start-error_2.csv\",\n",
    "    2: \"machine_ON_no-ref_start-error_3.csv\",\n",
    "    3: \"machine_ON_no-ref_start-error_4.csv\",\n",
    "    4: \"machine_ON_ref_no-error_1.csv\",  # Machine ON referenced and no-error idling\n",
    "    5: \"machine_ON_ref_no-error_2.csv\",  # Machine ON referenced and no-error idling\n",
    "    6: \"machine_ON_ref_no-error_3.csv\",\n",
    "    7: \"machine_ON_ref_no-error_4.csv\",\n",
    "    8: \"machine_ON_ref_no-error_5.csv\",\n",
    "    9: \"machine_ON_ref_no-error_6.csv\",\n",
    "    10: \"machine_ON_ref_no-error_7.csv\",\n",
    "    11: \"machine_ON_ref_no-error_8.csv\",\n",
    "    12: \"machine_ON_ref_no-error_9.csv\",\n",
    "    13: \"machine_ON_ref_no-error_10.csv\",\n",
    "    14: \"machine_ON_ref_overtravel-error_x_neg_1.csv\",  # Machine ON referenced and Overtravel for X negative\n",
    "    15: \"machine_ON_ref_overtravel-error_x_pos_1.csv\",  # Machine ON referenced and Overtravel for X positive\n",
    "    16: \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\",  # Machine ON not-referenced and Overtravel for X negative\n",
    "    17: \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", # Machine ON not-referenced and Overtravel for X positive\n",
    "    18: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\", # Reference and overtravel in X\n",
    "    19: \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", # Referenced and overtravel in X\n",
    "    20: \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\", # Referenced and overtravel in X\n",
    "    21: \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\",  # Machine ON referenced and Overtravel for Y negative\n",
    "    22: \"machine_ON_ref_overtravel-error_y_neg_1.csv\", # Machine and ON referenced and Overtravel in Y\n",
    "    23: \"machine_ON_ref_overtravel-error_y_pos_1.csv\",  # Machine ON referenced and Overtravel for Y positive\n",
    "    24: \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\",\n",
    "    25: \"machine_ON_ref_overtravel-error_z_neg_1.csv\",  # Machine ON referenced and Overtravel for Z negative\n",
    "    26: \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\",\n",
    "    27: \"machine_ON_ref_overtravel-error_z_pos_1.csv\",  # Machine ON referenced and Overtravel for Z positive\n",
    "    28: \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\",\n",
    "    29: \"machine_ON_no-ref_1.csv\",\n",
    "    30: \"machine_ON_no-ref_2.csv\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "index = 5\n",
    "df = pd.read_csv(os.path.join(data_loc, file_names[index]), header=\"infer\", index_col=\"no\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "df[\"PowerSum\"][-120:].plot(ax=axs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "\n",
    "- Segment\n",
    "- Identify the anomalous and non-anomalous class\n",
    "- Feature extraction\n",
    "- Generate training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_secs = 60\n",
    "wavelet_nperseg = 15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dont choose \"no\" and \"sample_time\" as they will be added later to the beginning\n",
    "# Chosen - Three different power components for three phases\n",
    "chosen_cols = [\"Power1\", \"Power2\", \"Power3\", \"PowerReac1\", \"PowerReac2\", \"PowerReac3\", \"PowerApp1\", \"PowerApp2\", \"PowerApp3\"]\n",
    "segmented_data = {}\n",
    "for index, file_name in file_names.items():\n",
    "    path = os.path.join(data_loc, file_name)\n",
    "    temp = data_prep.segment_data(file_name=path, col_names=chosen_cols, segment_secs=segment_secs)\n",
    "    # Remove the sample_time col\n",
    "    temp = temp[:, 1:, :]\n",
    "    segmented_data[file_name] =  temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that segmentation is successful\n",
    "for file_name in segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"For the file-{file_name} the shape-{segmented_data[file_name].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine classes\n",
    "\n",
    "- Anomaly - 0\n",
    "- Not Anomaly - 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Associations between the classes and the files in this study\n",
    "class_file_association = {\n",
    "    \"on-ref\": [\"machine_ON_ref_no-error_1.csv\", \"machine_ON_ref_no-error_2.csv\", \"machine_ON_ref_no-error_3.csv\", \"machine_ON_ref_no-error_4.csv\", \"machine_ON_ref_no-error_5.csv\", \"machine_ON_ref_no-error_6.csv\", \"machine_ON_ref_no-error_7.csv\", \"machine_ON_ref_no-error_8.csv\", \"machine_ON_ref_no-error_9.csv\", \"machine_ON_ref_no-error_10.csv\"],# \"machine_ON_no-ref_1.csv\", \"machine_ON_no-ref_2.csv\"],\n",
    "\n",
    "    \"on-noref-error\": [\"machine_ON_no-ref_start-error_1.csv\", \"machine_ON_no-ref_start-error_2.csv\", \"machine_ON_no-ref_start-error_3.csv\", \"machine_ON_no-ref_start-error_4.csv\"],\n",
    "\n",
    "    \"overtravel-x\": [\"machine_ON_ref_overtravel-error_x_neg_1.csv\", \"machine_ON_ref_overtravel-error_x_pos_1.csv\", \"machine_ON_no-ref_overtravel-error_x_neg_1.csv\", \"machine_ON_no-ref_overtravel-error_x_pos_1.csv\", \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_1.csv\",\n",
    "    \"machine_ON_ref_overtravel-error_x_neg_axes-extreme_2.csv\", \"machine_ON_ref_overtravel-error_x_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-y\": [\"machine_ON_ref_overtravel-error_y_neg_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_1.csv\",\n",
    "                    \"machine_ON_ref_overtravel-error_y_neg_axes-extreme_1.csv\", \"machine_ON_ref_overtravel-error_y_pos_axes-extreme_1.csv\"],\n",
    "\n",
    "    \"overtravel-z\": [\"machine_ON_ref_overtravel-error_z_neg_1.csv\", \"machine_ON_ref_overtravel-error_z_pos_1.csv\", \"machine_ON_ref_overtravel-error_z_neg_axes-extreme_1.csv\"] # , \"machine_ON_ref_overtravel-error_z_pos_axes-extreme_1.csv\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Okay\n",
    "class_segmented_data = {}\n",
    "for class_instance in class_file_association.keys():\n",
    "    for index, file_name in enumerate(class_file_association[class_instance]):\n",
    "\n",
    "        if index == 0:\n",
    "            class_segmented_data[class_instance] = segmented_data[file_name]\n",
    "        else:\n",
    "            class_segmented_data[class_instance] = np.append(class_segmented_data[class_instance], segmented_data[file_name], axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape the data appropriately\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    class_segmented_data[class_instance] = np.transpose(class_segmented_data[class_instance], (2, 1, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to ensure that the files have been loaded correctly\n",
    "for class_instance in class_segmented_data.keys():\n",
    "\n",
    "    sys.stdout.write(f\"The class-{class_instance} has the shape-{class_segmented_data[class_instance].shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "\n",
    "- Extract all the features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_dataset_features = {}\n",
    "for class_instance in class_segmented_data.keys():\n",
    "    dataset_features = []\n",
    "    for row in class_segmented_data[class_instance]:\n",
    "        computed_features = []\n",
    "        for col in row:\n",
    "            freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": wavelet_nperseg}]\n",
    "            freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "            # Extract all features\n",
    "            computed_features += feature_extraction.compute_all_features(col, freq_args=freq_args, freq_time_args=freq_time_args)\n",
    "\n",
    "        # Append to a list\n",
    "        dataset_features.append(computed_features)\n",
    "\n",
    "    # Add to class instance\n",
    "    class_dataset_features[class_instance] = np.array(dataset_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sys.stdout.write(\"After feature extraction process\\n\\n\")\n",
    "for class_instance in class_dataset_features.keys():\n",
    "\n",
    "    sys.stdout.write(f'For the class-{class_instance} , the extracted features has the shape={class_dataset_features[class_instance].shape}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model development\n",
    "\n",
    "- Estimating the mean and covariance of non-anomalous condition\n",
    "- Using kernels to estimate density"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the train and testing datasets\n",
    "X_train = class_dataset_features[\"on-ref\"]\n",
    "\n",
    "# Split the training dataset - Uncomment if needed\n",
    "# X_train, X_train_test = train_test_split(X_train, test_size=0.2, random_state=42)\n",
    "# Add the unseen normal class\n",
    "# X_test[\"normal-test\"] = X_train_test\n",
    "\n",
    "# Split it by different know classes\n",
    "X_test = {}\n",
    "for class_name in class_dataset_features.keys():\n",
    "    if class_name != \"on-ref\":\n",
    "        X_test[class_name] = class_dataset_features[class_name]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom DynamoDB Query\n",
    "\n",
    "- Select any data from table and make a query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data_loader_dynamodb import DynamoDBDataLoader\n",
    "\n",
    "start_time = int(time.time()) - 75\n",
    "end_time = int(time.time())\n",
    "\n",
    "# Instance\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=\"us-east-1\")\n",
    "ddb = DynamoDBDataLoader(table_name=\"robonano1_energy_wn\", region=\"us-east-1\", dynamodb=dynamodb)\n",
    "\n",
    "# Query\n",
    "ddb.query_data(sample_time_range=(start_time, end_time))\n",
    "# Get the dataframe\n",
    "ddb.get_dataframe()\n",
    "# get data\n",
    "data = ddb.data\n",
    "\n",
    "# Choose appropriate columns\n",
    "chosen_cols = [\"Power1\", \"Power2\", \"Power3\", \"PowerReac1\", \"PowerReac2\", \"PowerReac3\", \"PowerApp1\", \"PowerApp2\", \"PowerApp3\"]\n",
    "data = data[chosen_cols]\n",
    "data = data.to_numpy()[0:60, :]\n",
    "\n",
    "# Feature extraction\n",
    "# Apply col by col\n",
    "freq_args = [{\"axis\": 0}, {\"axis\": 0}, {\"axis\": 0, \"nperseg\": 30}]\n",
    "freq_time_args = [{\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}, {\"wavelet\": \"db1\"}]\n",
    "computed_features = []\n",
    "for col_index in range(data.shape[1]):\n",
    "    computed_features += feature_extraction.compute_all_features(data[:, col_index], freq_args, freq_time_args)\n",
    "# Convert from list to numpy array\n",
    "query_pred_data = np.array(computed_features)[np.newaxis, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA and Mahalanobis distance Anomaly detection\n",
    "\n",
    "- Reduce the dimension using PCA\n",
    "- Use mahalanobis distance as the metric to identify if things go out of distribution\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimating mean and covariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MahalanobisDistanceClassifer(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, threshold_level, **kwargs):\n",
    "\n",
    "        # Get the arguments\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # Training threshold level\n",
    "        self.threshold_level = threshold_level\n",
    "\n",
    "        # Training parameters\n",
    "        self.covariance = None\n",
    "        self.inv_covariance = None\n",
    "        self.mean = None\n",
    "        self.trained_threshold = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # Compute the centroid of the distribution\n",
    "        self.covariance = np.cov(X, rowvar=self.kwargs[\"rowvar\"]) if \"rowvar\" in self.kwargs.keys() else np.cov(X)\n",
    "        self.inv_covariance = np.linalg.inv(self.covariance)\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "\n",
    "        # Determine the threshold\n",
    "        training_distances = self.__distance_distribution(X, self.mean, self.inv_covariance)\n",
    "        self.trained_threshold = self.__compute_threshold(training_distances, level=self.threshold_level)\n",
    "\n",
    "    def predict(self, X, y=None, distance_type=\"mahalanobis\"):\n",
    "\n",
    "        # Compute the distance\n",
    "        if distance_type == \"mahalanobis\":\n",
    "            distances = self.__distance_distribution(X, self.mean, self.inv_covariance)\n",
    "            # Make predictions\n",
    "            distances = np.array(distances)\n",
    "            predictions = np.where(distances < self.trained_threshold, 0, 1)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Distance metric not implemented\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_threshold(distances, level):\n",
    "        # Compute mean and STD\n",
    "        normal_distances_mean = np.mean(distances)\n",
    "        normal_distances_std = np.std(distances)\n",
    "\n",
    "        return normal_distances_mean + (level * normal_distances_std)\n",
    "\n",
    "    @staticmethod\n",
    "    def __distance_metric(X, mean, inv_cov, metric_type=\"mahalanobis\"):\n",
    "\n",
    "        assert len(X.shape) == 2, \"The X for prediction must be an array, and not a vector\"\n",
    "\n",
    "        # Distance metric\n",
    "        if metric_type == \"mahalanobis\":\n",
    "            # difference\n",
    "            difference = (X - mean).T\n",
    "\n",
    "            return np.sqrt(difference.T.dot(inv_cov).dot(difference))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Metric not defined\")\n",
    "\n",
    "    def __distance_distribution(self, X, mean, inv_cov, metric_type=\"mahalanobis\"):\n",
    "\n",
    "        dd = []\n",
    "        for index, item in enumerate(X):\n",
    "            distance = self.__distance_metric(item[np.newaxis, :], mean, inv_cov, metric_type=metric_type).squeeze()\n",
    "            dd.append(distance.tolist())\n",
    "\n",
    "        return dd\n",
    "\n",
    "    def compute_distributions(self, X):\n",
    "\n",
    "        # Compute the distances for different data\n",
    "        distances = self.__distance_distribution(X, self.mean, self.inv_covariance)\n",
    "\n",
    "        return distances\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=140, svd_solver=\"full\")\n",
    "\n",
    "# Transform data\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "\n",
    "X_test_PCA = {}\n",
    "for class_name in X_test:\n",
    "    X_test_PCA[class_name] = pca.transform(X_test[class_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mahalanobis distance distribution\n",
    "- Distance from the center of the distribution for the non-anomalous data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model initialization and fitting\n",
    "md = MahalanobisDistanceClassifer(threshold_level=3, rowvar=False)\n",
    "# Fit the model\n",
    "md.fit(X_train_PCA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the distance for both Train and Test\n",
    "distances_training = md.compute_distributions(X_train_PCA)\n",
    "distances_testing = {}\n",
    "for class_name in X_test_PCA.keys():\n",
    "    distances_testing[class_name] = md.compute_distributions(X_test_PCA[class_name])\n",
    "\n",
    "# Merged dictionaries\n",
    "distributions = copy.deepcopy(distances_testing)\n",
    "distributions[\"normal\"] = distances_training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating figures\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# Plotting the distributions\n",
    "dist_plot = sns.kdeplot(data = distributions, fill=True, ax=axs, palette=\"pastel\")\n",
    "# Setting labels and properties\n",
    "dist_plot.set_xlabel(\"Mahalanobis Distance\")\n",
    "dist_plot.set_title(\"Mahalanobis distance distribution\")\n",
    "dist_plot.set_xlim([0, 250])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the combined anomalous class\n",
    "distances_testing_combined = []\n",
    "for class_name in distances_testing.keys():\n",
    "    distances_testing_combined += distances_testing[class_name]\n",
    "# Get the distributions\n",
    "distributions = dict()\n",
    "distributions[\"normal\"] = distances_training\n",
    "distributions[\"anomaly\"] = distances_testing_combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating figures\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "axs = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# Plotting the distributions\n",
    "dist_plot = sns.kdeplot(data = distributions, fill=True, ax=axs, palette=\"pastel\")\n",
    "# Setting labels and properties\n",
    "dist_plot.set_xlabel(\"Mahalanobis Distance\")\n",
    "dist_plot.set_title(\"Mahalanobis distance distribution\")\n",
    "dist_plot.set_xlim([0, 250])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determine Threshold\n",
    "\n",
    "- Generally 3$\\sigma$ from the mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the PCA to reduce dimension\n",
    "pca = PCA(n_components=140, svd_solver=\"full\")\n",
    "\n",
    "# Transform data\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "\n",
    "X_test_PCA = {}\n",
    "for class_name in X_test:\n",
    "    X_test_PCA[class_name] = pca.transform(X_test[class_name])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test the threshold\n",
    "\n",
    "- Apply and compare performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize evaluations\n",
    "kwargs = {\n",
    "    \"accuracy_score\": {},\n",
    "    \"balanced_accuracy_score\": {},\n",
    "    \"f1_score\": {},\n",
    "    \"recall_score\": {},\n",
    "    \"precision_score\": {},\n",
    "\n",
    "}\n",
    "\n",
    "# Dataset after PCA application\n",
    "threshold_levels = [1, 2, 3, 4]\n",
    "evaluations = {}\n",
    "for threshold_level in threshold_levels:\n",
    "    # Model initialization and fitting\n",
    "    md = MahalanobisDistanceClassifer(threshold_level=threshold_level, rowvar=False)\n",
    "    # Fit the model\n",
    "    md.fit(X_train_PCA)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred1 = md.predict(X_train_PCA)\n",
    "    y_true1 = np.repeat(0, len(y_pred1))\n",
    "    for index, class_name in enumerate(X_test_PCA.keys()):\n",
    "        temp = md.predict(X_test_PCA[class_name])\n",
    "        if index == 0:\n",
    "            y_pred2 = temp\n",
    "        else:\n",
    "            y_pred2 = np.concatenate([y_pred2, temp])\n",
    "    y_true2 = np.repeat(1, len(y_pred2))\n",
    "\n",
    "    # Performance metrics\n",
    "    model_eval = model_evaluations.ModelEval()\n",
    "    evaluations[round(md.trained_threshold, 2)] = copy.deepcopy(model_eval.compute_all_metrics(np.concatenate([y_true1, y_true2]), np.concatenate([y_pred1, y_pred2]), kwargs=kwargs))\n",
    "\n",
    "# Get it as dataFrame\n",
    "evaluations = pd.DataFrame(evaluations)\n",
    "evaluations = evaluations.T.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the evaluations\n",
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# Evaluation labels\n",
    "labels = [\"1$\\sigma$\", \"2$\\sigma$\", \"3$\\sigma$\", \"4$\\sigma$\"]\n",
    "width = 0.15\n",
    "x = np.arange(4)\n",
    "\n",
    "# Plotting\n",
    "ax.bar(x - 0.225, evaluations[\"accuracy_score\"], width, label=\"Accuracy Score\")\n",
    "ax.bar(x - 0.075, evaluations[\"f1_score\"], width, label=\"F1-Score\")\n",
    "ax.bar(x + 0.075, evaluations[\"precision_score\"], width, label=\"Precision Score\")\n",
    "ax.bar(x + 0.225, evaluations[\"recall_score\"], width, label=\"Recall Score\")\n",
    "\n",
    "ax.set_xlabel(\"Thresholds\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_title(\"\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single shot prediction\n",
    "\n",
    "- Visualize the outputs from prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do a single shot prediction\n",
    "md.predict(X_train_PCA[0][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline testing\n",
    "\n",
    "- Create and test pipelines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline of the estimator\n",
    "md_estimator = [\n",
    "    ('reduce_dim', pca),\n",
    "    ('clf', md)\n",
    "]\n",
    "md_pipeline = Pipeline(md_estimator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md_pipeline.predict(X_train[0][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom DynamoBD Test\n",
    "\n",
    "- Test a custom query"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md_pipeline.predict(query_pred_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md.predict(pca.transform(query_pred_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above two cells should have same results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel density estimation\n",
    "\n",
    "- Estimate the density of the normal class\n",
    "- Score and evaluate to determine if there is an anomaly in the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Kernel density estimation does not have a predict method, can I implement that"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KDEAnomalyDetector(KernelDensity):\n",
    "\n",
    "    def __init__(self, quantile_threshold, **kwargs):\n",
    "        super(KDEAnomalyDetector, self).__init__(**kwargs)\n",
    "\n",
    "        # Thresholds\n",
    "        self.trained_threshold = None\n",
    "        self.quantile_threshold = quantile_threshold\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "\n",
    "        # Fit the super class to the data\n",
    "        super(KDEAnomalyDetector, self).fit(X, y, sample_weight)\n",
    "\n",
    "        # Get the scores for the trained case\n",
    "        normal_scores = super(KDEAnomalyDetector, self).score_samples(X)\n",
    "        # Compute threshold from normal scores\n",
    "        self.trained_threshold = np.quantile(normal_scores, q=self.quantile_threshold)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Score the sample using the super class\n",
    "        scores = super(KDEAnomalyDetector, self).score_samples(X)\n",
    "        # Relative to threshold - make predictions\n",
    "        predictions = np.where(scores < self.trained_threshold, 1, 0)\n",
    "\n",
    "        return predictions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=140, svd_solver=\"full\")\n",
    "\n",
    "# Transform data\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "\n",
    "X_test_PCA = {}\n",
    "for class_name in X_test:\n",
    "    X_test_PCA[class_name] = pca.transform(X_test[class_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Estimate the density\n",
    "# kde = KernelDensity(kernel=\"gaussian\")\n",
    "kde = KDEAnomalyDetector(quantile_threshold=0.02, kernel=\"gaussian\")\n",
    "\n",
    "# Fit the training data\n",
    "kde.fit(X_train_PCA)\n",
    "\n",
    "# Get scores\n",
    "normal_predictions = kde.predict(X_train_PCA)\n",
    "anomaly_predictions = {}\n",
    "for class_name in X_test_PCA.keys():\n",
    "    anomaly_predictions[class_name] = kde.predict(X_test_PCA[class_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determine threshold\n",
    "\n",
    "- The threshold will be determined internally using the modified class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test the threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize evaluations\n",
    "kwargs = {\n",
    "    \"accuracy_score\": {},\n",
    "    \"balanced_accuracy_score\": {},\n",
    "    \"f1_score\": {},\n",
    "    \"recall_score\": {},\n",
    "    \"precision_score\": {},\n",
    "\n",
    "}\n",
    "\n",
    "# Combine all anomaly classes\n",
    "for index, class_name in enumerate(anomaly_predictions.keys()):\n",
    "    if index == 0:\n",
    "        temp = anomaly_predictions[class_name]\n",
    "    else:\n",
    "        temp = np.concatenate([temp, anomaly_predictions[class_name]])\n",
    "\n",
    "# Prediction\n",
    "# y_pred1 = np.where(normal_scores < threshold, 1, 0)\n",
    "y_pred1 = normal_predictions\n",
    "y_true1 = np.repeat(0, X_train_PCA.shape[0])\n",
    "# y_pred2 = np.where(temp < threshold, 1, 0)\n",
    "y_pred2 = temp\n",
    "y_true2 = np.repeat(1, temp.shape[0])\n",
    "\n",
    "# Performance metrics\n",
    "model_eval = model_evaluations.ModelEval()\n",
    "evaluations = model_eval.compute_all_metrics(np.concatenate([y_true1, y_true2]), np.concatenate([y_pred1, y_pred2]), kwargs=kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print out the evaluations\n",
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single shot prediction\n",
    "- To understand how the results look like\n",
    "- Prediction is usually a vector (n,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde.predict(X_train_PCA[0][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference\n",
    "- The performance was 100%. Needs to be tested with more new-normal scenarios\n",
    "    - The new normal does not work. The threshold is too tight\n",
    "- Have to modify the threshold in some way\n",
    "- Maybe sample all possible errors (overtravel-x, y, z etc.,) compute something that provides more space to the new normal\n",
    "\n",
    "**A big issue here that needs to be rectified**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline testing\n",
    "\n",
    "- Testing if the pipeline works\n",
    "- Create and predict on the training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline of the estimator\n",
    "kde_estimator = [\n",
    "    ('reduce_dim', pca),\n",
    "    ('clf', kde)\n",
    "]\n",
    "kde_pipeline = Pipeline(kde_estimator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde_pipeline.predict(X_train[0][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom DynamoBD Test\n",
    "\n",
    "- Test a custom query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde_pipeline.predict(query_pred_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde.score(pca.transform(query_pred_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Isolation Forest - Outlier Estimation\n",
    "\n",
    "- Using this because of High Dimensionality of the input dataset\n",
    "- Application of PCA is optional - Depends on model's performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=140, svd_solver=\"full\")\n",
    "\n",
    "# Training data\n",
    "X_train_isoforest = pca.fit_transform(X_train)\n",
    "# X_train_isoforest = X_train\n",
    "\n",
    "# Testing data\n",
    "X_test_isoforest = {}\n",
    "for class_name in X_test:\n",
    "    X_test_isoforest[class_name] = pca.transform(X_test[class_name])\n",
    "    # X_test_isoforest[class_name] = X_test[class_name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit only on the good state of the machine\n",
    "iso_forest = IsolationForest(n_estimators=1000, bootstrap=False, contamination=0.05)\n",
    "# Fit the training dataset\n",
    "iso_forest.fit(X_train_isoforest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction\n",
    "\n",
    "- Calling the predict method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predicting the training data again\n",
    "y_pred1 = iso_forest.predict(X_train_isoforest)\n",
    "y_true1 = np.repeat(0, X_train_isoforest.shape[0])\n",
    "temp_count = 0\n",
    "for index, class_name in enumerate(X_test_isoforest.keys()):\n",
    "    predictions = iso_forest.predict(X_test_isoforest[class_name])\n",
    "    if index == 0:\n",
    "        y_pred2 = predictions\n",
    "        temp_count += predictions.shape[0]\n",
    "    else:\n",
    "        y_pred2 = np.concatenate([y_pred2, predictions])\n",
    "        temp_count += predictions.shape[0]\n",
    "y_true2 = np.repeat(1, temp_count)\n",
    "\n",
    "# Replace -1/+1\n",
    "y_pred1 = np.where(y_pred1 == 1, 0, 1)\n",
    "y_pred2 = np.where(y_pred2 == 1, 0, 1)\n",
    "\n",
    "# Evaluation\n",
    "# Performance metrics\n",
    "model_eval = model_evaluations.ModelEval()\n",
    "evaluations = model_eval.compute_all_metrics(np.concatenate([y_true1, y_true2]), np.concatenate([y_pred1, y_pred2]), kwargs=kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the evaluations\n",
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performed reasonable well. Not as good as the Kernal Density Estimation. But reasonable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single shot prediction\n",
    "\n",
    "- To see the shape of the final output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make a single prediction\n",
    "iso_forest.predict(X_train_isoforest[100][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom DynamoBD Test\n",
    "\n",
    "- Test a custom query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA transform if required\n",
    "query_transformed_data = pca.transform(query_pred_data)\n",
    "# query_transformed_data =  query_pred_data\n",
    "\n",
    "# Make predictions\n",
    "iso_forest.predict(query_transformed_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOF (Local Outlier Factor)\n",
    "\n",
    "- Neighbors of 20\n",
    "- with novelty=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=140, svd_solver=\"full\")\n",
    "\n",
    "# Training data\n",
    "X_train_lof = pca.fit_transform(X_train)\n",
    "# X_train_lof = X_train\n",
    "\n",
    "# Testing data\n",
    "X_test_lof = {}\n",
    "for class_name in X_test:\n",
    "    X_test_lof[class_name] = pca.transform(X_test[class_name])\n",
    "    # X_test_lof[class_name] = X_test[class_name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "lof = LocalOutlierFactor(n_neighbors=50, novelty=True, contamination=0.05, leaf_size=100)\n",
    "# Fit on the training data - with or without PCA reduction\n",
    "lof.fit(X_train_lof)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize evaluations\n",
    "kwargs = {\n",
    "    \"accuracy_score\": {},\n",
    "    \"balanced_accuracy_score\": {},\n",
    "    \"f1_score\": {},\n",
    "    \"recall_score\": {},\n",
    "    \"precision_score\": {},\n",
    "\n",
    "}\n",
    "\n",
    "# Predicting the training data again\n",
    "y_pred1 = lof.predict(X_train_lof)\n",
    "y_true1 = np.repeat(0, X_train_lof.shape[0])\n",
    "temp_count = 0\n",
    "for index, class_name in enumerate(X_test_lof.keys()):\n",
    "    predictions = lof.predict(X_test_lof[class_name])\n",
    "    if index == 0:\n",
    "        y_pred2 = predictions\n",
    "        temp_count += predictions.shape[0]\n",
    "    else:\n",
    "        y_pred2 = np.concatenate([y_pred2, predictions])\n",
    "        temp_count += predictions.shape[0]\n",
    "y_true2 = np.repeat(1, temp_count)\n",
    "\n",
    "# Replace -1/+1\n",
    "y_pred1 = np.where(y_pred1 == 1, 0, 1)\n",
    "y_pred2 = np.where(y_pred2 == 1, 0, 1)\n",
    "\n",
    "# Evaluation\n",
    "# Performance metrics\n",
    "model_eval = model_evaluations.ModelEval()\n",
    "evaluations = model_eval.compute_all_metrics(np.concatenate([y_true1, y_true2]), np.concatenate([y_pred1, y_pred2]), kwargs=kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The performance is equivalent to Isolation Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single shot Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make a single prediction\n",
    "lof.predict(X_train_lof[100][np.newaxis, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom DynamoDB Test\n",
    "\n",
    "- Test a custom query"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA transform if required\n",
    "query_transformed_data = pca.transform(query_pred_data)\n",
    "# query_transformed_data =  query_pred_data\n",
    "\n",
    "# Make predictions\n",
    "lof.predict(query_transformed_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}